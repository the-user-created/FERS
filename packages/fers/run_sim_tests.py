import os
import subprocess
import filecmp
import shutil
import h5py
import numpy as np

# Define the base directory
TEST_DIR = os.path.abspath('./test/sim_tests/')
TOLERANCE = 0  # Use 0 for exact comparison or small value (e.g., 1e-6) for flexibility


def run_simulation(test_path: str) -> bool:
    """
    Run the FERS simulation for a given test case

    :param test_path: Path to the test directory
    :return: True if the simulation runs successfully, False otherwise
    """
    # Change to the test directory
    original_dir = os.getcwd()
    os.chdir(test_path)

    # Run FERS simulation
    input_file = os.path.join(test_path, 'input.fersxml')
    fers_executable = os.path.abspath('../../../build/src/fers')
    result = subprocess.run([fers_executable, input_file, '--log-level=DEBUG'], capture_output=True, text=True)

    # Return to the original directory
    os.chdir(original_dir)

    if result.returncode != 0:
        return False
    return True


def compare_hdf5_files(expected_file: str, generated_file: str) -> bool:
    """
    Compare two HDF5 files' datasets using a numerical tolerance.

    :param expected_file: The path to the expected HDF5 file.
    :param generated_file: The path to the generated HDF5 file.
    :return: True if the files match within the tolerance, False otherwise.
    """
    # Compare two HDF5 files' datasets using a numerical tolerance
    with h5py.File(expected_file, 'r') as expected, h5py.File(generated_file, 'r') as generated:
        for key in expected.keys():
            if key not in generated:
                print(f"Dataset {key} missing in generated file.")
                return False

            expected_data = expected[key][:]
            generated_data = generated[key][:]

            if not np.allclose(expected_data, generated_data, atol=TOLERANCE):
                print(f"Data mismatch in dataset {key}")
                return False
    return True


def compare_output(test_path: str) -> bool:
    """
    Compare the expected output with the generated output for a given test case

    :param test_path: The path to the test directory containing the expected output
    :return: True if the output matches the expected output, False otherwise
    """
    # Compare expected output with generated output
    expected_dir = os.path.join(test_path, 'expected_output')

    # Check if the expected output directory exists and is not empty
    if not os.path.exists(expected_dir) or not os.listdir(expected_dir):
        print(f"Test {test_path} failed: No expected output files found.")
        return False

    for root, _, files in os.walk(expected_dir):
        for file in files:
            expected_file = str(os.path.join(root, file))
            generated_file = os.path.join(test_path, file)

            if file.endswith('.h5'):
                if not compare_hdf5_files(expected_file, generated_file):
                    print(f"Test {test_path} failed: {file} output mismatch")
                    return False
            elif not filecmp.cmp(expected_file, generated_file, shallow=False):
                if file.endswith('.csv') and test_path.split('/')[-1] == 'test9':
                    print("Special case for test9: Due to multithreading, the order of the rows in the CSV file may change")
                    # Special case for test9: Due to multithreading, the order of the rows in the CSV file may change
                    # This is expected and the test should pass if the data is the same
                    with open(expected_file, 'r') as expected, open(generated_file, 'r') as generated:
                        expected_data_set = set(expected.readlines())
                        generated_data_set = set(generated.readlines())
                        if expected_data_set != generated_data_set:
                            print(f"Test {test_path} failed: {file} output mismatch")
                            return False
                        else:
                            print(f"Test {test_path} passed: {file} output match")
                else:
                    print(f"Test {test_path} failed: {file} output mismatch")
                    return False
    return True


def clean_up(test_path: str) -> None:
    """
    Clean up any generated files that are generated by this script in the test directory.

    :param test_path: The path to the test directory
    :return: None
    """
    # Clean up any generated files that are generated by this script
    for item in os.listdir(test_path):
        if item not in ['input.fersxml', 'waveform.csv', 'expected_output', 'chirp.h5', 'input2.fersxml',
                        'antenna_pattern.fersxml', 'antenna_gain_pattern.h5', 'rcs_data.fersxml', 'py_module.py']:
            path = os.path.join(test_path, item)
            if os.path.isdir(path):
                shutil.rmtree(path)
            else:
                os.remove(path)


def run_tests() -> None:
    """
    Run all simulation tests in the test directory.

    :return: None
    """
    passed_tests = 0

    test_cases = [d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))]
    if not test_cases:
        print("No test cases found.")
        exit(1)

    for test_name in test_cases:
        test_path = os.path.join(TEST_DIR, test_name)
        if run_simulation(test_path) and compare_output(test_path):
            print(f"Test {test_name} passed.")
            passed_tests += 1
        else:
            print(f"Test {test_name} failed.")
        clean_up(test_path)

    print(f"Passed {passed_tests} out of {len(os.listdir(TEST_DIR))} tests.")

    if passed_tests == len(os.listdir(TEST_DIR)):
        exit(0)
    else:
        exit(1)


if __name__ == '__main__':
    run_tests()